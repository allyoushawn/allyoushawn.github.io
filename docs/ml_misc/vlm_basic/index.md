---
title: Vision Language Models Basic
layout: default
parent: ML Misc.
nav_order: 40
---

The article is based on the following source:
- [Vision Language Models Explained](https://huggingface.co/blog/vlms)
- [nanoVLM: The simplest repository to train your VLM in pure PyTorch](https://huggingface.co/blog/nanovlm)

We will focus on the LLaVA variant in this article. 

# Vision language models
The term vision language models here we are referring to the LLMs with vision capability. For LLaVA variant VLM, it consists of a pretrained vision module and a pretrained LLM and combined the two with the projector and module called multimodal projector and some fine-tuning. The following figure clearly demonstrates the relations between the components.

![vlm_overview](/docs/ml_misc/vlm_basic/images/vlm_overview.png)

## Training process in high level
LLaVA introduces a module **multimodal projector** to project the vision input to the embedding space of LLM tokens. 

1. Multimodal projector training. Use GPT-4 to generate questions whose answer is the caption. Use the QA set to train the multimodal projector while freezing other modules
2. Instruction fine-tuning. Unfreeze the text decoder and fine tune the model with instruction and the corresponding answers
3. The image encoder is frozen during the whole process


# nanoVLM

Our Colab notebook [here](https://github.com/allyoushawn/jupyter_notebook_projects/blob/main/ml_misc/workable_nanoVLM.ipynb).

We use [nanoVLM](https://github.com/huggingface/nanoVLM) for understanding the details of training a VLM. We tweaked the [Colab notebook](https://colab.research.google.com/github/huggingface/nanoVLM/blob/main/nanoVLM.ipynb) shared by the author to make it runnable with the implementation code when we were studying it.

When we list the elements in a batch data, we see

- input_ids
- attention_mask
- images
- labels

## input_ids

When we print the content of an input_ids sequence, we see the following structure
```text
<im_end><im_end>...<im_start>user<row1_col1><image><image>...<row1_col2><image>...<image>Question...<im_end><im_start>assistant Answer: A<im_end><im_start>user...
```

- It would start with a sequence of `<im_end>`
- Following `<im_start>user` and a sequence of image related tokens
- After the image sequence token, it would have the question and following an `<im_end>`
- Next, start with `<im_start>assistant` and following the corresponding answer and another `<im_end>`
- The above is one back-and-forth between the user and the assistant. The following `<im_start>` would kick off another back-and-forth between the user and the assistant

![input_ids_example1](/docs/ml_misc/vlm_basic/images/input_ids_example1.png)


**Why there are multiple `<im_end>` at the beginning?**
- `<im_end>` is the padding token in this notebook
- We do the left side padding here
- However, if we do `print(tokenizer.padding_side)` we would see the padding side is `right`
- The reason is we don't use the tokenizer to do the padding. In the codebase we do the padding ourselves [here](https://github.com/allyoushawn/nanoVLM/blob/main/data/collators.py#L52).

**What are `user` and `assistant`?**
- The LLM training here follows a user-agent-conversation-like [chat template](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
- The dataset here is a VQA dataset and the questions would be treated as the users' requests while the answers are the agent's responses. [[code reference1]](https://github.com/allyoushawn/nanoVLM/blob/main/data/datasets.py#L25) [[chat template]](https://github.com/allyoushawn/nanoVLM/blob/main/models/config.py#L35)
- In this dataset, one image would have multiple questions. Therefore, we would see multiple turns of `<user>` and `<agnet>` while only have one image at the beginning.

**What is `<row_1_col_1><image><image>...`?**
- From the [config](https://github.com/allyoushawn/nanoVLM/blob/main/models/config.py#L42-L46) we see that an input image will be split into 16 regions max. For region, it would be segmented by the image encoder module into patches. `<row_1_col_1><image><image>...` indicating it is the region on the row 1 and column 1 and the `<image>` following are the representation for patches generated by the image encoder.
- In the notebook we visualize an example of one input image is split into 4 regions, as illustrated below

![img_regions_example](/docs/ml_misc/vlm_basic/images/img_region_example.png)

**How does the images got processed?**
- It's possible that each training instances contains more than one image
- The `_process_images` [function](https://github.com/allyoushawn/nanoVLM/blob/main/data/datasets.py#L38) would use the processor to convert each image into a list of regions. (It has `DynamicResize` and `SplitImage` capability as [here](https://github.com/allyoushawn/nanoVLM/blob/main/data/processors.py#L20).)
- The `get_image_string` [function](https://github.com/allyoushawn/nanoVLM/blob/main/data/processors.py#L27) would obtain the corresponding `image_string` based on the processed image

**How does the model resolve multiple images in one training instances?**
- We see there are three sequence of `<image>` tokens distributed in the decoded input ids. It means that in this data instance, there are three sets of VQA and each one involved with one image and multiple questions
The question is how does the model know which image to attend to when answering a question?
- We don't have answers at the moment.

## labels
When we print the label tokens, we would see that only the assistant's response will be used for loss computation. Other tokens would just be ignored. 

![labels_example](/docs/ml_misc/vlm_basic/images/labels_example.png)


## attention_mask
When we print out the attention masks, we would see that only the padding token `<im_end>` would be ignored. If the `<im_end>` is not padding, it would not be ignored.

![attention_mask_example](/docs/ml_misc/vlm_basic/images/attention_mask_example.png)

# Image encoder

In this section we would introduce the image encoder used in LLaVA. From the [LLaVA 1.5 paper](https://arxiv.org/pdf/2310.03744) it's suggesting that it's using the [openai/clip-vit-large-patch14-336](https://huggingface.co/openai/clip-vit-large-patch14-336).

Regarding CLIP, the reference sources are
- [Blog](https://openai.com/index/clip/)
- [Paper](https://huggingface.co/papers/2103.00020)

Key description from the paper
- We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet

The description above indicates that the CLIP embedding is the SOTA image representation at that time and the primary baselines it compared with in the paper is ImageNet.

## Background
- Previously, the vision models are built based on labeled dataset
   - VGG, ImageNet
- It requires specifying visual concept 
- Directly learning  the image representation from raw text is a promising alternative. It enables zero-shot transferable learning